# -*- coding: utf-8 -*-
"""HR Analytics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sX6DTiJlfVOCObCm79zCgRiotuYkgJhf
"""

!pip install shap

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
import shap

df = pd.read_csv('/content/WA_Fn-UseC_-HR-Employee-Attrition.csv')
df.head()

df = df.drop(['EmployeeCount', 'EmployeeNumber', 'Over18', 'StandardHours'], axis=1)

categorical_cols = df.select_dtypes(include='object').columns
le = LabelEncoder()
for col in categorical_cols:
    df[col] = le.fit_transform(df[col])

df.isnull().sum()

# Attrition Count
sns.countplot(data=df, x='Attrition')
plt.title('Attrition Count')

# Attrition by Department
sns.countplot(data=df, x='Department', hue='Attrition')
plt.title('Attrition by Department')

# Attrition by Salary Band (e.g., MonthlyIncome)
df['IncomeBand'] = pd.qcut(df['MonthlyIncome'], q=4)
sns.countplot(data=df, x='IncomeBand', hue='Attrition')
plt.xticks(rotation=45)
plt.title('Attrition by Income Band')

# Promotion vs Attrition
sns.countplot(data=df, x='YearsSinceLastPromotion', hue='Attrition')
plt.title('Promotion History vs Attrition')

X = df.drop('Attrition', axis=1)
y = df['Attrition']  # Already encoded (Yes = 1, No = 0)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""A. Logistic Regression

"""

df = df.drop(columns=['IncomeBand'])

X = df.drop('Attrition', axis=1)
y = df['Attrition']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Run Logistic Regression
lr = LogisticRegression(max_iter=1000)
lr.fit(X_train, y_train)
y_pred_lr = lr.predict(X_test)

print("Accuracy:", accuracy_score(y_test, y_pred_lr))
print(confusion_matrix(y_test, y_pred_lr))
print(classification_report(y_test, y_pred_lr))

"""B. Decision Tree

"""

dt = DecisionTreeClassifier(max_depth=5)
dt.fit(X_train, y_train)
y_pred_dt = dt.predict(X_test)

print("Decision Tree Accuracy:", accuracy_score(y_test, y_pred_dt))
print(confusion_matrix(y_test, y_pred_dt))
print(classification_report(y_test, y_pred_dt))

"""A. Initialize SHAP Explainer

"""

# A. Initialize SHAP Explainer

explainer = shap.TreeExplainer(dt)  # Use for tree models
shap_values = explainer.shap_values(X_test)

# Use the shap_values list directly and specify the class index
shap.summary_plot(shap_values, X_test, class_inds=1) # Class 1 = Attrition

shap.summary_plot(shap_values, X_test, plot_type="bar", class_inds=1)

shap.initjs()
i = 5
shap.force_plot(explainer.expected_value[1], shap_values[i, :, 1], X_test.iloc[i])

# %%
df = pd.read_csv('/content/WA_Fn-UseC_-HR-Employee-Attrition.csv')
df.head()
# %%
# Keep 'EmployeeNumber' for now, drop other unnecessary columns
df = df.drop(['EmployeeCount', 'Over18', 'StandardHours'], axis=1)

# %%
categorical_cols = df.select_dtypes(include='object').columns
le = LabelEncoder()
for col in categorical_cols:
    df[col] = le.fit_transform(df[col])
# %%
df.isnull().sum()

# %%
# Attrition Count
sns.countplot(data=df, x='Attrition')
plt.title('Attrition Count')
# %%
# Attrition by Department
sns.countplot(data=df, x='Department', hue='Attrition')
plt.title('Attrition by Department')

# %%
# Attrition by Salary Band (e.g., MonthlyIncome)
df['IncomeBand'] = pd.qcut(df['MonthlyIncome'], q=4)
sns.countplot(data=df, x='IncomeBand', hue='Attrition')
plt.xticks(rotation=45)
plt.title('Attrition by Income Band')
# %%
# Promotion vs Attrition
sns.countplot(data=df, x='YearsSinceLastPromotion', hue='Attrition')
plt.title('Promotion History vs Attrition')
# %%
# Define features X by dropping 'Attrition' and 'EmployeeNumber'
X = df.drop(['Attrition', 'EmployeeNumber'], axis=1)
y = df['Attrition']  # Already encoded (Yes = 1, No = 0)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# %% [markdown]
# A. Logistic Regression
#
# %%
# IncomeBand was added for plotting, drop it before model training if not used as a feature
# If you intended to use IncomeBand as a feature, remove this line and update the X definition
df = df.drop(columns=['IncomeBand'])

# Redefine X and y after dropping IncomeBand, ensuring 'EmployeeNumber' is still excluded from X
X = df.drop(['Attrition', 'EmployeeNumber'], axis=1)
y = df['Attrition']

# Re-split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# Run Logistic Regression
lr = LogisticRegression(max_iter=1000)
lr.fit(X_train, y_train)
y_pred_lr = lr.predict(X_test)

print("Accuracy:", accuracy_score(y_test, y_pred_lr))
print(confusion_matrix(y_test, y_pred_lr))
print(classification_report(y_test, y_pred_lr))

# %% [markdown]
# B. Decision Tree
#
# %%
dt = DecisionTreeClassifier(max_depth=5)
dt.fit(X_train, y_train)
y_pred_dt = dt.predict(X_test)

print("Decision Tree Accuracy:", accuracy_score(y_test, y_pred_dt))
print(confusion_matrix(y_test, y_pred_dt))
print(classification_report(y_test, y_pred_dt))

# %% [markdown]
# A. Initialize SHAP Explainer
#
# %%
# A. Initialize SHAP Explainer

explainer = shap.TreeExplainer(dt)  # Use for tree models
shap_values = explainer.shap_values(X_test)

# Use the shap_values list directly and specify the class index
shap.summary_plot(shap_values, X_test, class_inds=1) # Class 1 = Attrition
# %%
shap.summary_plot(shap_values, X_test, plot_type="bar", class_inds=1)
# %%
shap.initjs()
i = 5
shap.force_plot(explainer.expected_value[1], shap_values[i, :, 1], X_test.iloc[i])
# %%
# To include EmployeeNumber in the final CSV, get it from the original df
# using the index of X_test. The original df still contains EmployeeNumber
# since we didn't drop it in the initial step.
X_test['EmployeeNumber'] = df.loc[X_test.index, 'EmployeeNumber']
X_test['Attrition_Prediction'] = y_pred_dt
X_test['Attrition_Probability'] = dt.predict_proba(X_test.drop(['EmployeeNumber', 'Attrition_Prediction'], axis=1))[:,1]

X_test.to_csv("ML_Predictions.csv", index=False)

print("SHAP columns:", shap_values[1].shape[1])
print("X_test columns:", X_test_shap.shape[1])

print(X_train.shape)
print(X_train.columns.tolist())

# X = all features; y = target
X = df.drop('Attrition', axis=1)
y = df['Attrition']

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

from sklearn.tree import DecisionTreeClassifier

dt = DecisionTreeClassifier(max_depth=5)
dt.fit(X_train, y_train)

# Save column order used in training
model_features = X_train.columns.tolist()

import shap
import matplotlib.pyplot as plt

# Reuse same column order as training
X_test_shap = X_test[model_features].copy()

# Initialize SHAP
explainer = shap.TreeExplainer(dt)
# Pass the full shap_values array
shap_values = explainer.shap_values(X_test_shap)

# Final confirmation
# The shape of shap_values is (n_samples, n_features, n_classes)
print("shap_values shape:", shap_values.shape)
print("X_test_shap shape:", X_test_shap.shape)

# Beeswarm Plot
plt.figure()
# Pass the full shap_values and specify the class index for plotting
shap.summary_plot(shap_values, X_test_shap, class_inds=1, show=False)
plt.savefig("shap_beeswarm.png", bbox_inches='tight')

# Bar Plot
plt.figure()
# Pass the full shap_values and specify the class index for plotting
shap.summary_plot(shap_values, X_test_shap, plot_type="bar", class_inds=1, show=False)
plt.savefig("shap_bar.png", bbox_inches='tight')